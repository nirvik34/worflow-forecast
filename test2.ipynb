{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d0204e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 135\u001b[39m\n\u001b[32m    132\u001b[39m target_date_str = \u001b[33m\"\u001b[39m\u001b[33m2025-10-20\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     result = \u001b[43mpredict_future_cases\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_date_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalerX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalerY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Future Prediction ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    137\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[33m'\u001b[39m\u001b[33mrequested_target_date\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 109\u001b[39m, in \u001b[36mpredict_future_cases\u001b[39m\u001b[34m(problem_type, target_date_str, df, model, scalerX, scalerY, encoder, sequence_length)\u001b[39m\n\u001b[32m    106\u001b[39m X_scaled = scalerX.transform(X_seq_to_predict)\n\u001b[32m    107\u001b[39m X_input = np.array([X_scaled])\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m y_pred_scaled = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m predicted_cases = safe_numeric(scalerY.inverse_transform(y_pred_scaled)[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m], \u001b[32m0\u001b[39m)\n\u001b[32m    112\u001b[39m severity_factor = {\u001b[32m1\u001b[39m: \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m: \u001b[32m1.5\u001b[39m, \u001b[32m3\u001b[39m: \u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m: \u001b[32m3\u001b[39m}\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:505\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@traceback_utils\u001b[39m.filter_traceback\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mself\u001b[39m, x, batch_size=\u001b[38;5;28;01mNone\u001b[39;00m, verbose=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, steps=\u001b[38;5;28;01mNone\u001b[39;00m, callbacks=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    503\u001b[39m ):\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     epoch_iterator = \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module.CallbackList):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:728\u001b[39m, in \u001b[36mTFEpochIterator.__init__\u001b[39m\u001b[34m(self, distribute_strategy, *args, **kwargs)\u001b[39m\n\u001b[32m    726\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args, **kwargs)\n\u001b[32m    727\u001b[39m \u001b[38;5;28mself\u001b[39m._distribute_strategy = distribute_strategy\n\u001b[32m--> \u001b[39m\u001b[32m728\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf.distribute.DistributedDataset):\n\u001b[32m    730\u001b[39m     dataset = \u001b[38;5;28mself\u001b[39m._distribute_strategy.experimental_distribute_dataset(\n\u001b[32m    731\u001b[39m         dataset\n\u001b[32m    732\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:235\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    233\u001b[39m     indices_dataset = indices_dataset.map(tf.random.shuffle)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m dataset = \u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m options = tf.data.Options()\n\u001b[32m    238\u001b[39m options.experimental_distribute.auto_shard_policy = (\n\u001b[32m    239\u001b[39m     tf.data.experimental.AutoShardPolicy.DATA\n\u001b[32m    240\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:228\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[39m\u001b[34m(indices_dataset, inputs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shuffle:\n\u001b[32m    225\u001b[39m     options.experimental_external_state_policy = (\n\u001b[32m    226\u001b[39m         tf.data.experimental.ExternalStatePolicy.IGNORE\n\u001b[32m    227\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_options\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3012\u001b[39m, in \u001b[36mDatasetV2.with_options\u001b[39m\u001b[34m(self, options, name)\u001b[39m\n\u001b[32m   2986\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwith_options\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, name=\u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[33m\"\u001b[39m\u001b[33mDatasetV2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2987\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Returns a new `tf.data.Dataset` with the given options set.\u001b[39;00m\n\u001b[32m   2988\u001b[39m \n\u001b[32m   2989\u001b[39m \u001b[33;03m  The options are \"global\" in the sense they apply to the entire dataset.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3010\u001b[39m \u001b[33;03m    ValueError: when an option is set more than once to a non-default value\u001b[39;00m\n\u001b[32m   3011\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3012\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_OptionsDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:4909\u001b[39m, in \u001b[36m_OptionsDataset.__init__\u001b[39m\u001b[34m(self, input_dataset, options, name)\u001b[39m\n\u001b[32m   4907\u001b[39m \u001b[38;5;28mself\u001b[39m._name = name\n\u001b[32m   4908\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ops.colocate_with(input_dataset._variant_tensor):\n\u001b[32m-> \u001b[39m\u001b[32m4909\u001b[39m   variant_tensor = \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptions_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m      \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_pb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m      \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_common_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[38;5;28msuper\u001b[39m(_OptionsDataset, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m(input_dataset, variant_tensor)\n\u001b[32m   4914\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._options_attr:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Projects\\HACKATHONS\\SIH\\forecast2\\myvenv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:4663\u001b[39m, in \u001b[36moptions_dataset\u001b[39m\u001b[34m(input_dataset, serialized_options, output_types, output_shapes, metadata, name)\u001b[39m\n\u001b[32m   4661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   4662\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4663\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4664\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mOptionsDataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mserialized_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4665\u001b[39m \u001b[43m      \u001b[49m\u001b[43mserialized_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_types\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_shapes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4666\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4667\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   4668\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "# === Load Data & Models ===\n",
    "try:\n",
    "    df = pd.read_csv(\"data/dataset2.csv\")\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "\n",
    "    model = load_model(\"model/model.keras\", compile=False)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    scalerX = joblib.load(\"model/scaler_X.pkl\")\n",
    "    scalerY = joblib.load(\"model/scaler_Y.pkl\")\n",
    "    encoder = joblib.load(\"model/encoder.pkl\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    sys.exit(f\"[FATAL] Missing required file: {e.filename}. Please check dataset and model files.\")\n",
    "\n",
    "\n",
    "def safe_numeric(value, default=0.0):\n",
    "    \"\"\"Return value if it is finite, otherwise return default.\"\"\"\n",
    "    if pd.isna(value) or np.isinf(value):\n",
    "        return default\n",
    "    return value\n",
    "\n",
    "\n",
    "def predict_future_cases(problem_type, target_date_str, df, model, scalerX, scalerY, encoder, sequence_length=30):\n",
    "    \"\"\"\n",
    "    Predict future reported cases & workforce for a given problem type and date.\n",
    "    Handles missing values safely to avoid NaN -> int errors.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    problem_type_input = problem_type.strip().title()\n",
    "\n",
    "    try:\n",
    "        target_date = pd.Timestamp(target_date_str)\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Invalid date format. Use YYYY-MM-DD.\")\n",
    "\n",
    "    today = pd.Timestamp(datetime.today().date())\n",
    "    if target_date <= today:\n",
    "        raise ValueError(\"Target date must be in the future from today.\")\n",
    "    if (target_date - today).days > 60:\n",
    "        raise ValueError(\"Target date too far in future (>60 days). Limit your forecast window.\")\n",
    "\n",
    "    subset = df[df[\"problem_type\"] == problem_type_input].sort_values(\"date\").reset_index(drop=True)\n",
    "    if subset.empty:\n",
    "        raise ValueError(f\"No historical data for problem type '{problem_type_input}'.\")\n",
    "\n",
    "    # Fill missing numeric values in history to avoid NaNs\n",
    "    numeric_cols = [\"reported_cases\", \"workforce_required\", \"severity_score\", \n",
    "                    \"weather_score\", \"rainfall_mm\", \"problem_severity_interaction\"]\n",
    "    for col in numeric_cols:\n",
    "        if col in subset.columns:\n",
    "            subset[col] = subset[col].fillna(subset[col].median())  # safe fallback\n",
    "\n",
    "    seq = subset.tail(sequence_length).copy()\n",
    "    if len(seq) < sequence_length:\n",
    "        last_row = seq.iloc[-1].copy()\n",
    "        last_date = last_row[\"date\"]\n",
    "        for i in range(sequence_length - len(seq)):\n",
    "            new_row = last_row.copy()\n",
    "            new_row[\"date\"] = last_date + pd.Timedelta(days=(i + 1))\n",
    "            seq = pd.concat([seq, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    while target_date not in seq[\"date\"].values:\n",
    "        last_seq = seq.tail(sequence_length).copy()\n",
    "        next_date = seq[\"date\"].max() + pd.Timedelta(days=1)\n",
    "\n",
    "        new_row = last_seq.iloc[-1].copy()\n",
    "        new_row[\"date\"] = next_date\n",
    "        new_row[\"day_of_week\"] = next_date.weekday()\n",
    "        new_row[\"month\"] = next_date.month\n",
    "        new_row[\"is_weekend\"] = int(next_date.weekday() >= 5)\n",
    "        new_row[\"holiday_flag\"] = 0\n",
    "\n",
    "        prev_cases = safe_numeric(last_seq[\"reported_cases\"].iloc[-1], default=0)\n",
    "        avg_cases = safe_numeric(last_seq[\"reported_cases\"].tail(3).mean(), default=prev_cases)\n",
    "\n",
    "        new_row[\"prev_day_cases\"] = prev_cases\n",
    "        new_row[\"prev_3day_avg_cases\"] = avg_cases\n",
    "\n",
    "        new_row[\"severity_score\"] = safe_numeric(last_seq[\"severity_score\"].iloc[-1], 1)\n",
    "        new_row[\"weather_score\"] = np.clip(\n",
    "            safe_numeric(last_seq[\"weather_score\"].iloc[-1], 0) + np.random.normal(0, 0.05), 0.0, 1.0\n",
    "        )\n",
    "        new_row[\"rainfall_mm\"] = max(0.0, safe_numeric(last_seq[\"rainfall_mm\"].iloc[-1], 0) + np.random.normal(0, 2))\n",
    "        new_row[\"problem_severity_interaction\"] = safe_numeric(\n",
    "            last_seq[\"problem_severity_interaction\"].iloc[-1], 1\n",
    "        )\n",
    "\n",
    "        # === Prepare Model Input (Exactly Like Training) ===\n",
    "        X_seq_to_predict = last_seq.drop(columns=[\"date\", \"reported_cases\", \"workforce_required\"])\n",
    "        X_seq_to_predict = pd.concat([\n",
    "            X_seq_to_predict.drop(columns=[\"problem_type\", \"region\"]),\n",
    "            pd.DataFrame(\n",
    "                encoder.transform(X_seq_to_predict[[\"problem_type\", \"region\"]]),\n",
    "                columns=encoder.get_feature_names_out([\"problem_type\", \"region\"])\n",
    "            )\n",
    "        ], axis=1)\n",
    "\n",
    "        X_scaled = scalerX.transform(X_seq_to_predict)\n",
    "        X_input = np.array([X_scaled])\n",
    "\n",
    "        y_pred_scaled = model.predict(X_input, verbose=0)\n",
    "        predicted_cases = safe_numeric(scalerY.inverse_transform(y_pred_scaled)[0, 0], 0)\n",
    "\n",
    "        severity_factor = {1: 1, 2: 1.5, 3: 2, 4: 3}\n",
    "        sev = int(max(1, min(4, round(new_row[\"severity_score\"]))))\n",
    "        predicted_workforce = int(max(0, predicted_cases * severity_factor[sev]))\n",
    "\n",
    "        new_row[\"reported_cases\"] = predicted_cases\n",
    "        new_row[\"workforce_required\"] = predicted_workforce\n",
    "        seq = pd.concat([seq, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    target_row = seq[seq[\"date\"] == target_date].iloc[-1]\n",
    "\n",
    "    return {\n",
    "        \"requested_target_date\": target_date.strftime(\"%Y-%m-%d\"),\n",
    "        \"problem_type\": problem_type_input,\n",
    "        \"predicted_cases\": int(round(safe_numeric(target_row[\"reported_cases\"], 0))),\n",
    "        \"predicted_workforce\": int(round(safe_numeric(target_row[\"workforce_required\"], 0)))\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    problem_type = \"Garbage & Waste\"\n",
    "    target_date_str = \"2025-10-20\"\n",
    "\n",
    "    try:\n",
    "        result = predict_future_cases(problem_type, target_date_str, df, model, scalerX, scalerY, encoder)\n",
    "        print(\"\\n=== Future Prediction ===\")\n",
    "        print(f\"Date: {result['requested_target_date']}\")\n",
    "        print(f\"Problem Type: {result['problem_type']}\")\n",
    "        print(f\"Predicted Reported Cases: {result['predicted_cases']}\")\n",
    "        print(f\"Predicted Workforce Required: {result['predicted_workforce']}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Prediction failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
